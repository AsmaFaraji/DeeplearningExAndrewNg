{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pllt\n",
    "from utils import sigmoid, relu, relu_backward, sigmoid_backward\n",
    "from testCase import *\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layersize):\n",
    "    \n",
    "    \n",
    "    np.random.seed(2)\n",
    "    L = len(layersize)\n",
    "    parameters = dict()\n",
    "    for i in range(1, L):\n",
    "        parameters['W' + str(i)] = np.random.randn(layersize[i], layersize[i - 1]) * 0.01\n",
    "        parameters['b' + str(i)] = np.zeros((layersize[i], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param = initialize_parameters([20, 10, 5, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    Z = np.dot(W,A) + b\n",
    "    cache = (A, W,b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_forward_activation(AL_1,W,b, activation):\n",
    "\n",
    "    Z, linear_cache = linear_forward(AL_1,W,b)\n",
    "    if activation == \"relu\":\n",
    "        AL, activation_cache = relu(Z)\n",
    "    if activation == \"sigmoid\":\n",
    "        AL, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)    \n",
    "    return AL , cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.18524692,  3.25156647]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "AL_1, W, b = linear_forward_activation_testcase()\n",
    "Z ,cache = linear_forward_activation(AL_1, W, b,\"relu\")\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X,parameters ,hidden_layers):\n",
    "    \n",
    "    caches = []\n",
    "    A_prev = X\n",
    "    \n",
    "    \n",
    "    for i in range(1,hidden_layers):\n",
    "        A,cache = linear_forward_activation(A_prev,parameters['W' + str(i)],\n",
    "                                                                         parameters['b' + str(i)], \"relu\")\n",
    "        A_prev = A\n",
    "        caches.append(cache)\n",
    "    #output layer\n",
    "    AL, cache = linear_forward_activation(A_prev,parameters['W' + str(hidden_layers)],\n",
    "                                   parameters['b' + str(hidden_layers)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    \n",
    "    print AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.2870236   2.90357109  2.14481915  5.93033737]]\n"
     ]
    }
   ],
   "source": [
    "X,parameters ,hidden_layers = L_model_forward_testCase()\n",
    "L_model_forward(X,parameters ,hidden_layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost_function(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -1/m * np.sum(np.dot(Y.T, np.log(AL))  + np.sum(np.dot((1 - Y).T, np.log(1 - AL))))\n",
    "    \n",
    "    \n",
    "    cost = np.squeeze(cost) \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.1054217993\n"
     ]
    }
   ],
   "source": [
    "AL , y =  cost_function_testCase()\n",
    "print cost_function(AL, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, linear_cache):\n",
    "    m = dZ.shape[1]\n",
    "    \n",
    "    AL_1 , W , b = linear_cache\n",
    "    print AL_1.shape\n",
    "    dw = 1/ m * np.dot(dZ,AL_1.T)\n",
    "    db = 1 / m * np.sum(dZ, axis=1,keepdims=True)\n",
    "    dAL_1 = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dAL_1, dw , db  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 4)\n",
      "(array([[ 1.04232747, -0.16475573, -0.44525808,  0.38297651],\n",
      "       [-2.61586929,  2.07450183,  1.32425729, -2.87828788],\n",
      "       [ 3.34845405, -2.06325402, -1.62138188,  3.00082321]]), array([[-0.,  0., -0.],\n",
      "       [-0.,  0., -0.]]), array([[-0.],\n",
      "       [-0.]]))\n"
     ]
    }
   ],
   "source": [
    "dz , linear_cache = linear_backward_testCase()\n",
    "print linear_backward(dz , linear_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "   \n",
    "    (linear_cache, activation_cache) = cache   \n",
    "    if activation == 'sigmoid':\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        \n",
    "        dAL_1, dw, db = linear_backward(dZ, linear_cache)\n",
    "    elif activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dAL_1, dw, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dAL_1, dw, db "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, cache):\n",
    "    \n",
    "    grad = {}\n",
    "    L = len(cache)\n",
    "    dAL = - np.divide(Y, AL) - np.divide((1 - Y), (1 - AL))\n",
    "    current_cache = cache[L - 1]\n",
    "    \n",
    "    grad[\"dA\" + str(L)] , grad[\"dw\" + str(L)] , grad[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache,\"sigmoid\")\n",
    "    \n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = cache[l]\n",
    "        grad[\"dA\" + str(l + 1)] , grad[\"dw\" + str( l + 1)] , grad[\"db\" + str(l + 1)] = linear_activation_backward(grad[\"dA\" + str(l + 2)],\n",
    "        current_cache,'relu')\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz shappe  (1, 3)\n",
      "(2, 3)\n",
      "(5, 3)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "AL, Y, cache =  L_model_backward_testCase()\n",
    "print L_model_backward(AL, Y, cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grad, learning_rate):\n",
    "    for i in range(1,len(parameters) / 2 + 1):\n",
    "        parameters['W' + str(i)] =  parameters['W' + str(i)] - grad['dw' + str(i)] * learning_rate\n",
    "        parameters['b' + str(i)] =  parameters['b' + str(i)] - grad['db' + str(i)] * learning_rate\n",
    "        \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'b2': array([[-0.18508491],\n",
      "       [ 1.33753352]]), 'b1': array([[ 0.79318624],\n",
      "       [ 1.53861576],\n",
      "       [ 0.04503028]]), 'W1': array([[ 1.60976563,  0.39285887,  0.08684772, -1.67714343],\n",
      "       [-0.24964938, -0.31928308, -0.07446733, -0.56430061],\n",
      "       [-0.03943635, -0.42949623, -1.18247828,  0.79616014]]), 'W2': array([[-0.36420967, -0.49082395, -1.39182958],\n",
      "       [ 0.88413069, -0.99096087, -1.06654187]])}\n"
     ]
    }
   ],
   "source": [
    "parameters , grads = update_parameters_testCase()\n",
    "print update_parameters(parameters, grads , 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
